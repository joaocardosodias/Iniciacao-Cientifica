# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# Available providers: gemini, openai, anthropic, perplexity, deepseek, grok,
#                      mistral, cohere, groq, together, fireworks, cerebras, ollama

LLM_PROVIDER=gemini

# =============================================================================
# API KEYS
# =============================================================================

# Google Gemini
# Models: gemini-2.5-flash, gemini-2.0-flash-exp, gemini-1.5-pro
GOOGLE_API_KEY=your_key_here

# OpenAI
# Models: gpt-4o, gpt-4o-mini, gpt-4-turbo, o1-preview, o1-mini
OPENAI_API_KEY=your_key_here

# Anthropic Claude
# Models: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
ANTHROPIC_API_KEY=your_key_here

# Perplexity AI
# Models: llama-3.1-sonar-small-128k-online, llama-3.1-sonar-large-128k-online
PERPLEXITY_API_KEY=your_key_here

# DeepSeek
# Models: deepseek-chat, deepseek-coder
DEEPSEEK_API_KEY=your_key_here

# Grok (xAI)
# Models: grok-beta, grok-2-1212
XAI_API_KEY=your_key_here

# Mistral AI
# Models: mistral-large-latest, mistral-medium-latest, mistral-small-latest, codestral-latest
MISTRAL_API_KEY=your_key_here

# Cohere
# Models: command-r-plus, command-r, command-light
COHERE_API_KEY=your_key_here

# Groq (ultra-fast inference)
# Models: llama-3.3-70b-versatile, llama-3.1-8b-instant, mixtral-8x7b-32768
GROQ_API_KEY=your_key_here

# Together AI
# Models: meta-llama/Llama-3.3-70B-Instruct-Turbo, mistralai/Mixtral-8x22B-Instruct-v0.1
TOGETHER_API_KEY=your_key_here

# Fireworks AI
# Models: accounts/fireworks/models/llama-v3p1-70b-instruct
FIREWORKS_API_KEY=your_key_here

# Cerebras (ultra-fast inference)
# Models: llama3.1-70b, llama3.1-8b
CEREBRAS_API_KEY=your_key_here

# =============================================================================
# LOCAL MODELS
# =============================================================================

# Ollama (local)
# Models: llama3:70b, llama3:8b, mistral, codellama, phi3
OLLAMA_BASE_URL=http://localhost:11434
